#!/usr/bin/env python3
"""
Performance benchmark for M003 MIAIR Engine.

Tests document analysis and optimization performance against requirements:
- Process 100+ documents per minute
- Real-time quality scoring (<100ms per document)
- Incremental optimization support
"""

import time
import statistics
import random
import string
from typing import List, Dict, Any

# Add parent directory to path
import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from devdocai.miair.engine import MIAIREngine, MIAIRConfig
from devdocai.miair.entropy import ShannonEntropyCalculator
from devdocai.miair.scorer import QualityScorer
from devdocai.miair.optimizer import MIAIROptimizer
from devdocai.miair.patterns import PatternRecognizer


def generate_test_document(size: str = "medium") -> str:
    """Generate test document of specified size."""
    if size == "small":
        word_count = 100
    elif size == "medium":
        word_count = 500
    elif size == "large":
        word_count = 2000
    else:
        word_count = 500
    
    # Generate realistic document structure
    sections = [
        "# Document Title\n\n",
        "## Introduction\n\n",
        "This document provides information about the system. ",
        "It covers various aspects and features. ",
        "\n\n## Main Content\n\n"
    ]
    
    # Add random content
    words = ['system', 'process', 'data', 'user', 'interface', 'module', 
             'function', 'parameter', 'configuration', 'implementation']
    
    for _ in range(word_count // 10):
        sentence = ' '.join(random.choices(words, k=10)) + '. '
        sections.append(sentence)
    
    sections.append("\n\n## Conclusion\n\nThis concludes the documentation.")
    
    return ''.join(sections)


def benchmark_entropy_calculator():
    """Benchmark Shannon entropy calculations."""
    print("\n" + "="*60)
    print("BENCHMARK: Shannon Entropy Calculator")
    print("="*60)
    
    calculator = ShannonEntropyCalculator()
    
    # Test different document sizes
    sizes = ["small", "medium", "large"]
    results = {}
    
    for size in sizes:
        doc = generate_test_document(size)
        times = []
        
        # Run multiple iterations
        for _ in range(100):
            start = time.perf_counter()
            entropy = calculator.calculate_entropy(doc, 'all')
            elapsed = time.perf_counter() - start
            times.append(elapsed * 1000)  # Convert to ms
        
        avg_time = statistics.mean(times)
        std_dev = statistics.stdev(times)
        
        results[size] = {
            'avg_time_ms': avg_time,
            'std_dev_ms': std_dev,
            'ops_per_sec': 1000 / avg_time,
            'doc_length': len(doc)
        }
        
        print(f"\n{size.upper()} documents ({len(doc)} chars):")
        print(f"  Average time: {avg_time:.2f}ms")
        print(f"  Std deviation: {std_dev:.2f}ms")
        print(f"  Throughput: {1000/avg_time:.1f} ops/sec")
    
    return results


def benchmark_quality_scorer():
    """Benchmark quality scoring system."""
    print("\n" + "="*60)
    print("BENCHMARK: Quality Scorer")
    print("="*60)
    
    scorer = QualityScorer()
    
    sizes = ["small", "medium", "large"]
    results = {}
    
    for size in sizes:
        doc = generate_test_document(size)
        times = []
        
        for _ in range(100):
            start = time.perf_counter()
            metrics = scorer.score_document(doc)
            elapsed = time.perf_counter() - start
            times.append(elapsed * 1000)
        
        avg_time = statistics.mean(times)
        
        results[size] = {
            'avg_time_ms': avg_time,
            'ops_per_sec': 1000 / avg_time,
            'quality_score': metrics.overall
        }
        
        print(f"\n{size.upper()} documents:")
        print(f"  Average time: {avg_time:.2f}ms")
        print(f"  Throughput: {1000/avg_time:.1f} ops/sec")
        print(f"  Quality score: {metrics.overall:.2f}")
    
    # Check if meets <100ms requirement
    if all(r['avg_time_ms'] < 100 for r in results.values()):
        print("\n‚úÖ PASSED: All scoring operations < 100ms")
    else:
        print("\n‚ùå FAILED: Some scoring operations > 100ms")
    
    return results


def benchmark_pattern_recognition():
    """Benchmark pattern recognition."""
    print("\n" + "="*60)
    print("BENCHMARK: Pattern Recognition")
    print("="*60)
    
    recognizer = PatternRecognizer(learning_enabled=False)
    
    doc = generate_test_document("medium")
    times = []
    
    for _ in range(50):
        start = time.perf_counter()
        analysis = recognizer.analyze(doc)
        elapsed = time.perf_counter() - start
        times.append(elapsed * 1000)
    
    avg_time = statistics.mean(times)
    pattern_count = len(analysis.patterns)
    
    print(f"\nMedium documents:")
    print(f"  Average time: {avg_time:.2f}ms")
    print(f"  Patterns found: {pattern_count}")
    print(f"  Throughput: {1000/avg_time:.1f} ops/sec")
    
    return {'avg_time_ms': avg_time, 'pattern_count': pattern_count}


def benchmark_optimizer():
    """Benchmark optimization engine."""
    print("\n" + "="*60)
    print("BENCHMARK: Optimization Engine")
    print("="*60)
    
    from devdocai.miair.optimizer import OptimizationConfig
    
    opt_config = OptimizationConfig(
        max_iterations=3,
        timeout_seconds=5.0,
        target_quality=0.7
    )
    
    optimizer = MIAIROptimizer(opt_config)
    
    # Test with poor quality document
    doc = """
    guide
    
    TODO: add content
    
    install: run setup
    """
    
    times = []
    improvements = []
    
    for _ in range(10):  # Fewer iterations due to longer runtime
        start = time.perf_counter()
        result = optimizer.optimize_document(doc)
        elapsed = time.perf_counter() - start
        times.append(elapsed * 1000)
        improvements.append(result.improvement_percentage())
    
    avg_time = statistics.mean(times)
    avg_improvement = statistics.mean(improvements)
    
    print(f"\nOptimization results:")
    print(f"  Average time: {avg_time:.2f}ms")
    print(f"  Average improvement: {avg_improvement:.1f}%")
    print(f"  Iterations: {result.iterations}")
    
    return {
        'avg_time_ms': avg_time,
        'avg_improvement': avg_improvement,
        'iterations': result.iterations
    }


def benchmark_full_engine():
    """Benchmark complete MIAIR Engine."""
    print("\n" + "="*60)
    print("BENCHMARK: Full MIAIR Engine")
    print("="*60)
    
    config = MIAIRConfig(
        max_iterations=2,
        optimization_timeout=3.0,
        storage_enabled=False
    )
    
    engine = MIAIREngine(config)
    
    # Generate batch of documents
    documents = [generate_test_document("medium") for _ in range(10)]
    
    # Benchmark analysis
    print("\n1. Document Analysis:")
    analysis_times = []
    
    for doc in documents:
        start = time.perf_counter()
        analysis = engine.analyze_document(doc)
        elapsed = time.perf_counter() - start
        analysis_times.append(elapsed * 1000)
    
    avg_analysis = statistics.mean(analysis_times)
    print(f"  Average time: {avg_analysis:.2f}ms")
    print(f"  Throughput: {1000/avg_analysis:.1f} docs/sec")
    
    # Benchmark optimization
    print("\n2. Document Optimization:")
    opt_times = []
    
    for doc in documents[:5]:  # Use fewer for optimization
        start = time.perf_counter()
        result = engine.optimize_document(doc)
        elapsed = time.perf_counter() - start
        opt_times.append(elapsed)
    
    avg_opt = statistics.mean(opt_times)
    print(f"  Average time: {avg_opt:.2f}s")
    print(f"  Throughput: {60/avg_opt:.1f} docs/min")
    
    # Benchmark batch processing
    print("\n3. Batch Processing:")
    start = time.perf_counter()
    batch_result = engine.process_batch(documents, optimize=False)
    batch_time = time.perf_counter() - start
    
    print(f"  Total time: {batch_time:.2f}s")
    print(f"  Documents: {batch_result.total_documents}")
    print(f"  Throughput: {batch_result.total_documents/batch_time:.1f} docs/sec")
    
    # Check performance requirements
    docs_per_minute = (60 / avg_opt) * 5  # Extrapolate from 5 docs
    
    print("\n" + "="*60)
    print("PERFORMANCE REQUIREMENTS CHECK:")
    print("="*60)
    
    if docs_per_minute >= 100:
        print(f"‚úÖ PASSED: {docs_per_minute:.0f} docs/min (target: 100+)")
    else:
        print(f"‚ùå FAILED: {docs_per_minute:.0f} docs/min (target: 100+)")
    
    if avg_analysis < 100:
        print(f"‚úÖ PASSED: {avg_analysis:.1f}ms scoring (target: <100ms)")
    else:
        print(f"‚ùå FAILED: {avg_analysis:.1f}ms scoring (target: <100ms)")
    
    return {
        'analysis_time_ms': avg_analysis,
        'optimization_time_s': avg_opt,
        'batch_time_s': batch_time,
        'docs_per_minute': docs_per_minute
    }


def main():
    """Run all benchmarks."""
    print("\n" + "="*60)
    print("M003 MIAIR ENGINE PERFORMANCE BENCHMARKS")
    print("="*60)
    print("\nRunning comprehensive performance tests...")
    
    # Run individual component benchmarks
    entropy_results = benchmark_entropy_calculator()
    scorer_results = benchmark_quality_scorer()
    pattern_results = benchmark_pattern_recognition()
    optimizer_results = benchmark_optimizer()
    
    # Run full engine benchmark
    engine_results = benchmark_full_engine()
    
    # Summary
    print("\n" + "="*60)
    print("BENCHMARK SUMMARY")
    print("="*60)
    
    print("\nüìä Component Performance:")
    print(f"  Entropy Calculator: {entropy_results['medium']['ops_per_sec']:.0f} ops/sec")
    print(f"  Quality Scorer: {scorer_results['medium']['ops_per_sec']:.0f} ops/sec")
    print(f"  Pattern Recognition: {1000/pattern_results['avg_time_ms']:.0f} ops/sec")
    print(f"  Optimizer: {optimizer_results['avg_improvement']:.1f}% avg improvement")
    
    print("\nüöÄ System Performance:")
    print(f"  Document Analysis: {engine_results['analysis_time_ms']:.1f}ms")
    print(f"  Document Optimization: {engine_results['optimization_time_s']:.1f}s")
    print(f"  Throughput: {engine_results['docs_per_minute']:.0f} docs/min")
    
    print("\n‚úÖ M003 MIAIR Engine Pass 1 benchmarks complete!")


if __name__ == "__main__":
    main()