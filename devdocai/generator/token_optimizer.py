"""
Token optimization system for AI Document Generator.

Implements prompt compression, redundancy removal, and smart truncation
to achieve 30-50% token reduction without quality loss.
"""

import re
import logging
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
import tiktoken
from collections import Counter
import hashlib
from functools import lru_cache

logger = logging.getLogger(__name__)


@dataclass
class TokenStats:
    """Token usage statistics."""
    original_tokens: int
    optimized_tokens: int
    reduction_percentage: float
    estimated_cost_savings: float
    optimization_techniques: List[str]


class TokenOptimizer:
    """
    Advanced token optimization for LLM prompts.
    
    Features:
    - Prompt compression techniques
    - Redundant content removal
    - Smart truncation
    - Reference-based compression
    - Context prioritization
    """
    
    def __init__(
        self,
        target_reduction: float = 0.35,
        max_tokens: int = 4000,
        preserve_quality: bool = True
    ):
        """
        Initialize token optimizer.
        
        Args:
            target_reduction: Target reduction percentage (0.35 = 35%)
            max_tokens: Maximum tokens per prompt
            preserve_quality: Whether to preserve content quality
        """
        self.target_reduction = target_reduction
        self.max_tokens = max_tokens
        self.preserve_quality = preserve_quality
        
        # Initialize tokenizer (using cl100k_base for GPT-4/Claude compatibility)
        try:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
        except Exception:
            # Fallback to simple estimation if tiktoken not available
            self.tokenizer = None
            logger.warning("tiktoken not available, using estimation")
        
        # Optimization statistics
        self.stats = {
            "total_original": 0,
            "total_optimized": 0,
            "prompts_processed": 0
        }
        
        # Common abbreviations for technical terms
        self.abbreviations = {
            "software requirements specification": "SRS",
            "application programming interface": "API",
            "user interface": "UI",
            "user experience": "UX",
            "database": "DB",
            "artificial intelligence": "AI",
            "machine learning": "ML",
            "continuous integration": "CI",
            "continuous deployment": "CD",
            "development operations": "DevOps",
            "quality assurance": "QA",
            "return on investment": "ROI",
            "key performance indicator": "KPI",
            "software development kit": "SDK",
            "integrated development environment": "IDE"
        }
        
        logger.info(f"Initialized TokenOptimizer with target_reduction={target_reduction}")
    
    def count_tokens(self, text: str) -> int:
        """Count tokens in text."""
        if self.tokenizer:
            return len(self.tokenizer.encode(text))
        else:
            # Simple estimation: ~4 characters per token
            return len(text) // 4
    
    @lru_cache(maxsize=128)
    def _compute_content_hash(self, content: str) -> str:
        """Compute hash for content deduplication."""
        return hashlib.md5(content.encode()).hexdigest()[:8]
    
    def optimize_prompt(
        self,
        prompt: str,
        context: Optional[Dict[str, Any]] = None,
        priority_sections: Optional[List[str]] = None
    ) -> Tuple[str, TokenStats]:
        """
        Optimize a prompt to reduce token count.
        
        Args:
            prompt: Original prompt
            context: Additional context that might be compressed
            priority_sections: Sections to prioritize (keep intact)
            
        Returns:
            Tuple of (optimized_prompt, statistics)
        """
        original_tokens = self.count_tokens(prompt)
        optimized = prompt
        techniques_applied = []
        
        # Step 1: Remove redundant whitespace
        optimized = self._compress_whitespace(optimized)
        if optimized != prompt:
            techniques_applied.append("whitespace_compression")
        
        # Step 2: Apply abbreviations
        optimized = self._apply_abbreviations(optimized)
        if self.abbreviations and any(long in prompt.lower() for long in self.abbreviations.keys()):
            techniques_applied.append("abbreviations")
        
        # Step 3: Remove redundant context
        optimized = self._remove_redundant_context(optimized, context)
        if context:
            techniques_applied.append("context_deduplication")
        
        # Step 4: Compress repeated patterns
        optimized = self._compress_patterns(optimized)
        techniques_applied.append("pattern_compression")
        
        # Step 5: Smart truncation if still over limit
        if self.count_tokens(optimized) > self.max_tokens:
            optimized = self._smart_truncate(optimized, priority_sections)
            techniques_applied.append("smart_truncation")
        
        # Step 6: Structure optimization
        optimized = self._optimize_structure(optimized)
        techniques_applied.append("structure_optimization")
        
        # Calculate statistics
        optimized_tokens = self.count_tokens(optimized)
        reduction = 1 - (optimized_tokens / original_tokens) if original_tokens > 0 else 0
        
        # Estimate cost savings (rough estimate: $0.01 per 1K tokens)
        cost_per_1k = 0.01
        tokens_saved = original_tokens - optimized_tokens
        cost_savings = (tokens_saved / 1000) * cost_per_1k
        
        # Update global stats
        self.stats["total_original"] += original_tokens
        self.stats["total_optimized"] += optimized_tokens
        self.stats["prompts_processed"] += 1
        
        stats = TokenStats(
            original_tokens=original_tokens,
            optimized_tokens=optimized_tokens,
            reduction_percentage=reduction * 100,
            estimated_cost_savings=cost_savings,
            optimization_techniques=techniques_applied
        )
        
        logger.info(
            f"Optimized prompt: {original_tokens} â†’ {optimized_tokens} tokens "
            f"({reduction*100:.1f}% reduction, ${cost_savings:.4f} saved)"
        )
        
        return optimized, stats
    
    def _compress_whitespace(self, text: str) -> str:
        """Compress excessive whitespace."""
        # Replace multiple spaces with single space
        text = re.sub(r'\s+', ' ', text)
        # Remove trailing whitespace from lines
        text = '\n'.join(line.rstrip() for line in text.split('\n'))
        # Replace multiple newlines with double newline
        text = re.sub(r'\n{3,}', '\n\n', text)
        return text.strip()
    
    def _apply_abbreviations(self, text: str) -> str:
        """Apply common abbreviations to reduce token count."""
        if not self.preserve_quality:
            # Aggressive abbreviation
            for long_form, abbrev in self.abbreviations.items():
                # Case-insensitive replacement
                pattern = re.compile(re.escape(long_form), re.IGNORECASE)
                text = pattern.sub(abbrev, text)
        else:
            # Conservative abbreviation - only in non-critical sections
            lines = text.split('\n')
            for i, line in enumerate(lines):
                # Don't abbreviate in headers or important sections
                if not (line.startswith('#') or line.startswith('**') or ':' in line[:20]):
                    for long_form, abbrev in self.abbreviations.items():
                        line = re.sub(rf'\b{re.escape(long_form)}\b', abbrev, line, flags=re.IGNORECASE)
                lines[i] = line
            text = '\n'.join(lines)
        
        return text
    
    def _remove_redundant_context(self, prompt: str, context: Optional[Dict[str, Any]]) -> str:
        """Remove redundant context that appears multiple times."""
        if not context:
            return prompt
        
        # Find repeated blocks
        lines = prompt.split('\n')
        seen_hashes = set()
        filtered_lines = []
        reference_map = {}
        
        for line in lines:
            if len(line) > 50:  # Only consider substantial lines
                line_hash = self._compute_content_hash(line)
                
                if line_hash in seen_hashes:
                    # Replace with reference if seen before
                    if line_hash not in reference_map:
                        reference_map[line_hash] = f"[REF_{len(reference_map)+1}]"
                    
                    if self.preserve_quality:
                        # Keep first occurrence, replace others with reference
                        filtered_lines.append(f"{reference_map[line_hash]} (see above)")
                    else:
                        # Skip repeated content entirely
                        continue
                else:
                    seen_hashes.add(line_hash)
                    filtered_lines.append(line)
            else:
                filtered_lines.append(line)
        
        return '\n'.join(filtered_lines)
    
    def _compress_patterns(self, text: str) -> str:
        """Compress repeated patterns in text."""
        # Find repeated phrases (3+ words appearing 3+ times)
        words = text.split()
        
        if len(words) < 100:
            return text  # Too short to benefit from compression
        
        # Find repeated 3-word phrases
        phrases = []
        for i in range(len(words) - 2):
            phrase = ' '.join(words[i:i+3])
            if len(phrase) > 15:  # Meaningful phrases only
                phrases.append(phrase)
        
        # Count frequencies
        phrase_counts = Counter(phrases)
        
        # Replace frequent phrases with shorter references
        for phrase, count in phrase_counts.items():
            if count >= 3:
                # Create abbreviation from first letters
                abbrev = ''.join(word[0].upper() for word in phrase.split())
                if len(abbrev) < len(phrase) / 3:  # Significant reduction
                    # Add definition at start
                    definition = f"[{abbrev} = {phrase}]"
                    if definition not in text[:100]:
                        text = definition + "\n" + text
                    # Replace occurrences
                    text = text.replace(phrase, abbrev)
        
        return text
    
    def _smart_truncate(
        self,
        text: str,
        priority_sections: Optional[List[str]] = None
    ) -> str:
        """
        Intelligently truncate text while preserving important sections.
        
        Args:
            text: Text to truncate
            priority_sections: Sections to preserve (keywords/headers)
            
        Returns:
            Truncated text
        """
        if self.count_tokens(text) <= self.max_tokens:
            return text
        
        priority_sections = priority_sections or []
        lines = text.split('\n')
        
        # Score each line by importance
        scored_lines = []
        for i, line in enumerate(lines):
            score = 0
            
            # Higher score for priority sections
            for priority in priority_sections:
                if priority.lower() in line.lower():
                    score += 10
            
            # Higher score for headers
            if line.startswith('#'):
                score += 5
            
            # Higher score for lists
            if line.strip().startswith(('-', '*', '1.', '2.')):
                score += 2
            
            # Higher score for key patterns
            if any(pattern in line.lower() for pattern in ['must', 'required', 'important', 'critical']):
                score += 3
            
            # Lower score for examples and verbose sections
            if any(pattern in line.lower() for pattern in ['example:', 'for instance', 'such as']):
                score -= 2
            
            scored_lines.append((score, i, line))
        
        # Sort by score (descending) but maintain relative order for same scores
        scored_lines.sort(key=lambda x: (-x[0], x[1]))
        
        # Build truncated text
        selected_indices = set()
        current_tokens = 0
        
        for score, idx, line in scored_lines:
            line_tokens = self.count_tokens(line)
            if current_tokens + line_tokens <= self.max_tokens * 0.9:  # Leave 10% buffer
                selected_indices.add(idx)
                current_tokens += line_tokens
        
        # Reconstruct in original order
        truncated = []
        for i, line in enumerate(lines):
            if i in selected_indices:
                truncated.append(line)
            elif i > 0 and i-1 in selected_indices and i < len(lines)-1 and i+1 in selected_indices:
                # Keep single lines between selected content for continuity
                truncated.append(line)
        
        result = '\n'.join(truncated)
        
        # Add truncation notice if significant content removed
        if len(truncated) < len(lines) * 0.7:
            result += "\n\n[Note: Content truncated for token optimization]"
        
        return result
    
    def _optimize_structure(self, text: str) -> str:
        """Optimize text structure for efficiency."""
        # Convert verbose structures to concise ones
        replacements = [
            # Verbose connectors to concise
            (r'\bIn order to\b', 'To'),
            (r'\bAs a result of\b', 'Due to'),
            (r'\bIn the event that\b', 'If'),
            (r'\bAt this point in time\b', 'Now'),
            (r'\bIn spite of the fact that\b', 'Although'),
            
            # Remove filler phrases
            (r'\bIt is important to note that\b', ''),
            (r'\bIt should be noted that\b', ''),
            (r'\bIt goes without saying that\b', ''),
            (r'\bFor all intents and purposes\b', ''),
            
            # Simplify common patterns
            (r'\bwhether or not\b', 'whether'),
            (r'\beach and every\b', 'every'),
            (r'\bfirst and foremost\b', 'first'),
        ]
        
        for pattern, replacement in replacements:
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        # Remove empty lines created by replacements
        lines = [line for line in text.split('\n') if line.strip()]
        
        return '\n'.join(lines)
    
    def optimize_context(
        self,
        context: Dict[str, Any],
        max_context_tokens: int = 1000
    ) -> Dict[str, Any]:
        """
        Optimize context dictionary to reduce tokens.
        
        Args:
            context: Context dictionary
            max_context_tokens: Maximum tokens for context
            
        Returns:
            Optimized context
        """
        optimized = {}
        
        # Prioritize context keys
        priority_keys = ['user_stories', 'requirements', 'objectives', 'constraints']
        
        # Add priority keys first
        current_tokens = 0
        for key in priority_keys:
            if key in context:
                value_str = str(context[key])
                value_tokens = self.count_tokens(value_str)
                
                if current_tokens + value_tokens <= max_context_tokens:
                    optimized[key] = context[key]
                    current_tokens += value_tokens
                else:
                    # Truncate if needed
                    remaining = max_context_tokens - current_tokens
                    if remaining > 100:  # Only add if meaningful
                        truncated = self._smart_truncate(value_str, [key])
                        optimized[key] = truncated
                        current_tokens = max_context_tokens
                        break
        
        # Add other keys if space remains
        for key, value in context.items():
            if key not in optimized and current_tokens < max_context_tokens:
                value_str = str(value)
                value_tokens = self.count_tokens(value_str)
                
                if current_tokens + value_tokens <= max_context_tokens:
                    optimized[key] = value
                    current_tokens += value_tokens
        
        return optimized
    
    def batch_optimize(
        self,
        prompts: List[str],
        shared_context: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[str, TokenStats]]:
        """
        Optimize multiple prompts with shared context.
        
        Args:
            prompts: List of prompts to optimize
            shared_context: Context shared across prompts
            
        Returns:
            List of (optimized_prompt, stats) tuples
        """
        results = []
        
        # Optimize shared context once
        if shared_context:
            shared_context = self.optimize_context(shared_context)
        
        # Process each prompt
        for prompt in prompts:
            optimized, stats = self.optimize_prompt(prompt, shared_context)
            results.append((optimized, stats))
        
        return results
    
    def get_optimization_stats(self) -> Dict[str, Any]:
        """Get overall optimization statistics."""
        if self.stats["prompts_processed"] > 0:
            avg_reduction = 1 - (
                self.stats["total_optimized"] / 
                self.stats["total_original"]
            ) if self.stats["total_original"] > 0 else 0
            
            return {
                "prompts_processed": self.stats["prompts_processed"],
                "total_tokens_saved": self.stats["total_original"] - self.stats["total_optimized"],
                "average_reduction": avg_reduction * 100,
                "total_cost_savings": (
                    (self.stats["total_original"] - self.stats["total_optimized"]) / 1000 * 0.01
                )
            }
        
        return self.stats


class StreamingOptimizer:
    """
    Optimizer for streaming token generation.
    
    Enables progressive rendering and reduces perceived latency.
    """
    
    def __init__(self, chunk_size: int = 50):
        """
        Initialize streaming optimizer.
        
        Args:
            chunk_size: Tokens per streaming chunk
        """
        self.chunk_size = chunk_size
        self.buffer = []
        
    async def stream_optimized_response(
        self,
        response_generator,
        optimize_func=None
    ):
        """
        Stream optimized response chunks.
        
        Args:
            response_generator: Async generator of response chunks
            optimize_func: Optional function to optimize each chunk
            
        Yields:
            Optimized response chunks
        """
        async for chunk in response_generator:
            # Apply optimization if provided
            if optimize_func:
                chunk = optimize_func(chunk)
            
            # Buffer management for smooth streaming
            self.buffer.append(chunk)
            
            # Yield when buffer reaches chunk size
            if len(self.buffer) >= self.chunk_size:
                yield ''.join(self.buffer)
                self.buffer.clear()
        
        # Yield remaining buffer
        if self.buffer:
            yield ''.join(self.buffer)
            self.buffer.clear()


# Singleton instance
_token_optimizer: Optional[TokenOptimizer] = None


def get_token_optimizer() -> TokenOptimizer:
    """Get or create singleton token optimizer instance."""
    global _token_optimizer
    if _token_optimizer is None:
        _token_optimizer = TokenOptimizer()
    return _token_optimizer